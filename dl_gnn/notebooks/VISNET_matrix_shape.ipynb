{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00032c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4fd0f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai2md_path = r\"C:\\Users\\23174\\Desktop\\GitHub Project\\AI2BMD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6d037835",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(ai2md_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7af84bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger, TensorBoardLogger\n",
    "from pytorch_lightning.strategies import DDPStrategy\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "\n",
    "from visnet import datasets, models, priors\n",
    "from visnet.data import DataModule\n",
    "from visnet.models import output_modules\n",
    "from visnet.models.utils import act_class_mapping, rbf_class_mapping\n",
    "from visnet.module import LNNP\n",
    "from visnet.utils import LoadFromCheckpoint, LoadFromFile, number, save_argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee41b35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1\n"
     ]
    }
   ],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description='Training')\n",
    "    parser.add_argument('--load-model', action=LoadFromCheckpoint, help='Restart training using a model checkpoint')  # keep first\n",
    "    parser.add_argument('--conf', '-c', type=open, action=LoadFromFile, help='Configuration yaml file')  # keep second\n",
    "    \n",
    "    # training settings\n",
    "    parser.add_argument('--num-epochs', default=300, type=int, help='number of epochs')\n",
    "    parser.add_argument('--lr-warmup-steps', type=int, default=0, help='How many steps to warm-up over. Defaults to 0 for no warm-up')\n",
    "    parser.add_argument('--lr', default=1e-4, type=float, help='learning rate')\n",
    "    parser.add_argument('--lr-patience', type=int, default=10, help='Patience for lr-schedule. Patience per eval-interval of validation')\n",
    "    parser.add_argument('--lr-min', type=float, default=1e-6, help='Minimum learning rate before early stop')\n",
    "    parser.add_argument('--lr-factor', type=float, default=0.8, help='Minimum learning rate before early stop')\n",
    "    parser.add_argument('--weight-decay', type=float, default=0.0, help='Weight decay strength')\n",
    "    parser.add_argument('--early-stopping-patience', type=int, default=30, help='Stop training after this many epochs without improvement')\n",
    "    parser.add_argument('--loss-type', type=str, default='MSE', choices=['MSE', 'MAE'], help='Loss type')\n",
    "    parser.add_argument('--loss-scale-y', type=float, default=1.0, help=\"Scale the loss y of the target\")\n",
    "    parser.add_argument('--loss-scale-dy', type=float, default=1.0, help=\"Scale the loss dy of the target\")\n",
    "    parser.add_argument('--energy-weight', default=1.0, type=float, help='Weighting factor for energies in the loss function')\n",
    "    \n",
    "    # dataset specific\n",
    "    parser.add_argument('--dataset', default=None, type=str, choices=datasets.__all__, help='Name of the torch_geometric dataset')\n",
    "    parser.add_argument('--dataset-arg', default=None, type=str, help='Additional dataset argument')\n",
    "    parser.add_argument('--dataset-root', default=None, type=str, help='Data storage directory')\n",
    "    parser.add_argument('--derivative', default=False, action=argparse.BooleanOptionalAction, help='If true, take the derivative of the prediction w.r.t coordinates')\n",
    "    parser.add_argument('--split-mode', default=None, type=str, help='Split mode for Molecule3D dataset')\n",
    "    \n",
    "    # dataloader specific\n",
    "    parser.add_argument('--reload', type=int, default=0, help='Reload dataloaders every n epoch')\n",
    "    parser.add_argument('--batch-size', default=32, type=int, help='batch size')\n",
    "    parser.add_argument('--inference-batch-size', default=None, type=int, help='Batchsize for validation and tests.')\n",
    "    parser.add_argument('--standardize', action=argparse.BooleanOptionalAction, default=False, help='If true, multiply prediction by dataset std and add mean')\n",
    "    parser.add_argument('--splits', default=None, help='Npz with splits idx_train, idx_val, idx_test')\n",
    "    parser.add_argument('--train-size', type=number, default=950, help='Percentage/number of samples in training set (None to use all remaining samples)')\n",
    "    parser.add_argument('--val-size', type=number, default=50, help='Percentage/number of samples in validation set (None to use all remaining samples)')\n",
    "    parser.add_argument('--test-size', type=number, default=None, help='Percentage/number of samples in test set (None to use all remaining samples)')\n",
    "    parser.add_argument('--num-workers', type=int, default=4, help='Number of workers for data prefetch')\n",
    "    \n",
    "    # model architecture specific\n",
    "    parser.add_argument('--model', type=str, default='ViSNetBlock', choices=models.__all__, help='Which model to train')\n",
    "    parser.add_argument('--output-model', type=str, default='Scalar', choices=output_modules.__all__, help='The type of output model')\n",
    "    parser.add_argument('--prior-model', type=str, default=None, choices=priors.__all__, help='Which prior model to use')\n",
    "    parser.add_argument('--prior-args', type=dict, default=None, help='Additional arguments for the prior model')\n",
    "    \n",
    "    # architectural specific\n",
    "    parser.add_argument('--embedding-dimension', type=int, default=256, help='Embedding dimension')\n",
    "    parser.add_argument('--num-layers', type=int, default=6, help='Number of interaction layers in the model')\n",
    "    parser.add_argument('--num-rbf', type=int, default=64, help='Number of radial basis functions in model')\n",
    "    parser.add_argument('--activation', type=str, default='silu', choices=list(act_class_mapping.keys()), help='Activation function')\n",
    "    parser.add_argument('--rbf-type', type=str, default='expnorm', choices=list(rbf_class_mapping.keys()), help='Type of distance expansion')\n",
    "    parser.add_argument('--trainable-rbf', action=argparse.BooleanOptionalAction, default=False, help='If distance expansion functions should be trainable')\n",
    "    parser.add_argument('--attn-activation', default='silu', choices=list(act_class_mapping.keys()), help='Attention activation function')\n",
    "    parser.add_argument('--num-heads', type=int, default=8, help='Number of attention heads')\n",
    "    parser.add_argument('--cutoff', type=float, default=5.0, help='Cutoff in model')\n",
    "    parser.add_argument('--max-z', type=int, default=100, help='Maximum atomic number that fits in the embedding matrix')\n",
    "    parser.add_argument('--max-num-neighbors', type=int, default=32, help='Maximum number of neighbors to consider in the network')\n",
    "    parser.add_argument('--reduce-op', type=str, default='add', choices=['add', 'mean'], help='Reduce operation to apply to atomic predictions')\n",
    "    parser.add_argument('--lmax', type=int, default=2, help='Max order of spherical harmonics')\n",
    "    parser.add_argument('--vecnorm-type', type=str, default='max_min', help='Type of vector normalization')\n",
    "    parser.add_argument('--trainable-vecnorm', action=argparse.BooleanOptionalAction, default=False, help='If vector normalization should be trainable')\n",
    "    parser.add_argument('--vertex-type', type=str, default='Edge', choices=['None', 'Edge', 'Node'], help='If add vertex angle and Where to add vertex angles')\n",
    "\n",
    "    # other specific\n",
    "    parser.add_argument('--ngpus', type=int, default=-1, help='Number of GPUs, -1 use all available. Use CUDA_VISIBLE_DEVICES=1, to decide gpus')\n",
    "    parser.add_argument('--num-nodes', type=int, default=1, help='Number of nodes')\n",
    "    parser.add_argument('--precision', type=int, default=32, choices=[16, 32], help='Floating point precision')\n",
    "    parser.add_argument('--log-dir', type=str, default=None, help='Log directory')\n",
    "    parser.add_argument('--task', type=str, default='train', choices=['train', 'inference'], help='Train or inference') \n",
    "    parser.add_argument('--seed', type=int, default=1, help='random seed (default: 1)')\n",
    "    parser.add_argument('--distributed-backend', default='ddp', help='Distributed backend')\n",
    "    parser.add_argument('--redirect', action=argparse.BooleanOptionalAction, default=False, help='Redirect stdout and stderr to log_dir/log')\n",
    "    parser.add_argument('--accelerator', default='gpu', help='Supports passing different accelerator types (\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"auto\")')\n",
    "    parser.add_argument('--test-interval', type=int, default=10, help='Test interval, one test per n epochs (default: 10)')\n",
    "    parser.add_argument('--save-interval', type=int, default=10, help='Save interval, one save per n epochs (default: 10)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.redirect:\n",
    "        os.makedirs(args.log_dir, exist_ok=True)\n",
    "        sys.stdout = open(os.path.join(args.log_dir, \"log\"), \"w\")\n",
    "        sys.stderr = sys.stdout\n",
    "        logging.getLogger(\"pytorch_lightning\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "    if args.inference_batch_size is None:\n",
    "        args.inference_batch_size = args.batch_size\n",
    "    save_argparse(args, os.path.join(args.log_dir, \"input.yaml\"), exclude=[\"conf\"])\n",
    "    \n",
    "    return args\n",
    "args = {\n",
    "    \"load_model\": None,\n",
    "    \"conf\": \"examples/ViSNet-MD17.yml\",\n",
    "    \"conf\": None,\n",
    "    \"num_epochs\": 300,\n",
    "    \"lr_warmup_steps\": 0,\n",
    "    \"lr\": 1e-4,\n",
    "    \"lr_patience\": 10,\n",
    "    \"lr_min\": 1e-6,\n",
    "    \"lr_factor\": 0.8,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"early_stopping_patience\": 30,\n",
    "    \"loss_type\": \"MSE\",\n",
    "    \"loss_scale_y\": 1.0,\n",
    "    \"loss_scale_dy\": 1.0,\n",
    "    \"energy_weight\": 1.0,\n",
    "    \"dataset\": datasets.__all__,\n",
    "    \"dataset_arg\": \"aspirin\",\n",
    "    \"dataset_root\": \"data\",\n",
    "    \"derivative\": False,\n",
    "    \"split_mode\": None,\n",
    "    \"reload\": 0,\n",
    "    \"batch_size\": 17,\n",
    "    \"inference_batch_size\": 17,  # Inferred default from batch_size as specified\n",
    "    \"standardize\": False,\n",
    "    \"splits\": None,\n",
    "    \"train_size\": 950,\n",
    "    \"val_size\": 50,\n",
    "    \"test_size\": None,\n",
    "    \"num_workers\": 4,\n",
    "    \"model\": \"ViSNetBlock\",\n",
    "    \"output_model\": \"Scalar\",\n",
    "    \"prior_model\": None,\n",
    "    \"prior_args\": None,\n",
    "    \"embedding_dimension\": 256,\n",
    "    \"num_layers\": 6,\n",
    "    \"num_rbf\": 64,\n",
    "    \"activation\": \"silu\",\n",
    "    \"rbf_type\": \"expnorm\",\n",
    "    \"trainable_rbf\": False,\n",
    "    \"attn_activation\": \"silu\",\n",
    "    \"num_heads\": 8,\n",
    "    \"cutoff\": 5.0,\n",
    "    \"max_z\": 100,\n",
    "    \"max_num_neighbors\": 32,\n",
    "    \"reduce_op\": \"add\",\n",
    "    \"lmax\": 2,\n",
    "    \"vecnorm_type\": \"max_min\",\n",
    "    \"trainable_vecnorm\": False,\n",
    "    \"vertex_type\": \"Edge\",\n",
    "    \"ngpus\": 1,\n",
    "    \"num_nodes\": 1,\n",
    "    \"precision\": 32,\n",
    "    \"log_dir\": \"logs\",\n",
    "    \"task\": \"train\",\n",
    "    \"seed\": 1,\n",
    "    \"distributed_backend\": \"ddp\",\n",
    "    \"redirect\": False,\n",
    "    \"accelerator\": \"gpu\",\n",
    "    \"test_interval\": 10,\n",
    "    \"save_interval\": 10\n",
    "}\n",
    "\n",
    "\n",
    "pl.seed_everything(args['seed'], workers=True)\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "34439604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(self):\n",
    "    \n",
    "    # assert hasattr(self, f\"_prepare_{self.hparams['dataset']}_dataset\"), f\"Dataset {self.hparams['dataset']} not defined\"\n",
    "    dataset_factory = lambda t: getattr(self, f\"_prepare_MD17_dataset\")()\n",
    "    self.idx_train, self.idx_val, self.idx_test = dataset_factory(self.hparams[\"dataset\"])\n",
    "    print(self.dataset)\n",
    "    self.train_dataset = Subset(self.dataset, self.idx_train)\n",
    "    self.val_dataset = Subset(self.dataset, self.idx_val)\n",
    "    self.test_dataset = Subset(self.dataset, self.idx_test)\n",
    "\n",
    "    if self.hparams[\"standardize\"]:\n",
    "        self._standardize()\n",
    "        \n",
    "\n",
    "DataModule.prepare_dataset = prepare_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc704eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "085ced40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MD17(211762)\n"
     ]
    }
   ],
   "source": [
    "# initialize data module\n",
    "data = DataModule(args)\n",
    "data._prepare_MD17_dataset()\n",
    "data.prepare_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ff37a8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Atomref']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "priors.__all__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa2939b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from visnet.priors import Atomref\n",
    "prior = None\n",
    "\n",
    "model = LNNP(args, prior_model=prior, mean=data.mean, std=data.std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "60ebe04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取训练数据加载器\n",
    "train_loader = data.train_dataloader()\n",
    "\n",
    "# 迭代训练数据加载器\n",
    "for idx, batch in enumerate(train_loader):\n",
    "    # 在这里处理每一个批次的数据\n",
    "    # `batch`通常包含两部分：输入数据和目标（标签）\n",
    "    if idx == 0:\n",
    "        batch_one = batch\n",
    "    elif idx == 1:\n",
    "        batch_two = batch\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d23c1c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(y=[17, 1], pos=[357, 3], z=[357], dy=[357, 3], batch=[357], ptr=[18])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "480132d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.data.batch.DataBatch"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batch_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad663194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "         1,  1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
       "         2,  2,  2,  2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
       "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,\n",
       "         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  5,  5,  5,\n",
       "         5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
       "         6,  6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "         7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "         8,  8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
       "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10,\n",
       "        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11,\n",
       "        11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,\n",
       "        12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "        12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
       "        13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "        14, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "        15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 16, 16, 16, 16, 16, 16,\n",
       "        16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch. batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ed270",
   "metadata": {},
   "outputs": [],
   "source": [
    "_1, _2 = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e96e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "357//17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2052cad",
   "metadata": {},
   "source": [
    "# # gnn lf load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1966629",
   "metadata": {
    "is_executing": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pathlib\n",
    "import sys\n",
    "model_config_json_path = \"model_config.json\"\n",
    "model_config = json.loads(pathlib.Path(\"model_config.json\").read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf7076ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_gnn_path = r\"C:\\Users\\23174\\Desktop\\GitHub Project\\GitHubProjectBigData\\GNN-Molecular-Project\\GNN-LF-AND-ColfNet\"\n",
    "dl_gnn_path_test = r\"C:\\Users\\23174\\Desktop\\GitHub Project\\GitHubProjectBigData\\GNN-Molecular-Project\\GNN-LF-AND-ColfNet\\tests\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24b93f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(dl_gnn_path)\n",
    "sys.path.append(dl_gnn_path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5376d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from colorama import Fore, Back, Style\n",
    "from dl_gnn.models.impl.GNNLF import GNNLF\n",
    "from dl_gnn.models.impl.ThreeDimFrame import GNNLF as ThreeDGNNLF\n",
    "from dl_gnn.configs.path_configs import OUTPUT_PATH\n",
    "from dl_gnn.models.impl import Utils\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from torch.nn.functional import l1_loss, mse_loss\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from warnings import filterwarnings\n",
    "from loguru import logger\n",
    "\n",
    "# from dl_gnn.tests.main_md17 import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c3c48f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.load('train_dataset.pth')\n",
    "validation_dataset = torch.load('validation_dataset.pth')\n",
    "test_dataset = torch.load('test_dataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "859d4386",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=17, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "13edab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_train_batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50418987",
   "metadata": {},
   "outputs": [],
   "source": [
    "z, pos, y, dy = first_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82bd59bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "57a5f59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([357, 3])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.view(-1, pos.size(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "478446a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 21])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c7ed6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([357])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.view(-1, ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e17d3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 21, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9606bbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([357, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy.view(-1, dy.size(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09030dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,  21,  42,  63,  84, 105, 126, 147, 168, 189, 210, 231, 252, 273,\n",
       "        294, 315, 336, 357])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sep = pos.shape[1]\n",
    "_batch_size = pos.shape[0]\n",
    "_ptr = torch.arange(0, _batch_size * sep + 1, sep)\n",
    "_ptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0358cc30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 21, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "668ba0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_gnn.models.visnet.models import output_modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05192085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99f5239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnnlf_model = GNNLF(**model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "df816d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Final\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn import ModuleList\n",
    "from dl_gnn.models.impl.Mol2Graph import Mol2Graph\n",
    "from dl_gnn.models.impl.Utils import innerprod\n",
    "from torch import Tensor\n",
    "\n",
    "def forward(self, atomic_numbers: Tensor, atomic_positions: Tensor):\n",
    "    \"\"\"\n",
    "    神经网络前向传播\n",
    "    :param atomic_numbers: the atomic number of each atom in the molecule, shape: batch_size x atomic_number\n",
    "    :param atomic_positions:  the coordinate of each atom in the molecule, shape: batch_size x atomic_number x 3\n",
    "    :return:\n",
    "        - atomic_number_embedding: Embeddings of atomic numbers with layer normalization applied.\n",
    "                                        Shape: (batch_size, atomic_number, hid_dim)\n",
    "        - atomic_adjacency_matrix: Smoothed adjacency matrix representing atomic connections.\n",
    "                                        Shape: (batch_size, atomic_number, atomic_number)\n",
    "        - normalized_atom_position_distances: Normalized vectors representing interatomic distances.\n",
    "                                                    Shape: (batch_size, atomic_number, atomic_number, 3)\n",
    "        - edge_features: Edge features computed using RBF, representing interatomic relationships.\n",
    "                                Shape: (batch_size, atomic_number, atomic_number, ef_dim)\n",
    "    \"\"\"\n",
    "    (\n",
    "        atomic_number_embedding,\n",
    "        atomic_adjacency_matrix,\n",
    "        normalized_atom_position_distances,\n",
    "        edge_features\n",
    "    ) = (\n",
    "        self.mol2graph(atomic_numbers, atomic_positions)\n",
    "    )\n",
    "    mask = self.ef_proj(edge_features) * atomic_adjacency_matrix.unsqueeze(-1)\n",
    "    #  通过一层linear， 再让其特征增加， 并且与领接矩阵相乘， 使得其与领接矩阵的维度一致。\n",
    "    #  unsqueeze(-1) 表示在最后一个维度上增加一个维度， 这里是增加一个维度， 使得其与mask的维度一致。\n",
    "    #  mask 的维度为 batch_size x atomic_number x atomic_number x hid_dim\n",
    "    s = self.neighbor_emb(atomic_numbers, atomic_number_embedding, mask)\n",
    "    #  s的维度为 batch_size x atomic_number x hid_dim\n",
    "    v = self.s2v(s, normalized_atom_position_distances, mask)\n",
    "    #  v的维度为 batch_size x atomic_number x 3 x hid_dim\n",
    "    if self.global_frame:\n",
    "        v = torch.sum(v, dim=1, keepdim=True).expand(-1, s.shape[1], -1, -1)\n",
    "    atomic_direction_feature_list = []\n",
    "\n",
    "    # TODO: 将colfnet的关于局部坐标框架的特征添投影到hidden_dim的维度上\n",
    "    # local_frame_featutes = self.localframe_features(position_batch_center=atomic_positions,\n",
    "    #                                                 atomic_adjacency_matrix=atomic_adjacency_matrix)\n",
    "    # projected_local_frame_featutes = self.colfnet_features_projection(local_frame_featutes)\n",
    "    # # #\n",
    "    # if self.colfnet_features:\n",
    "    #     atomic_direction_feature_list.append(projected_local_frame_featutes)\n",
    "    #\n",
    "    # if self.colfnet_features:\n",
    "    #     atomic_direction_feature_list.append(local_frame_featutes)\n",
    "    if self.use_dir1:\n",
    "        atomic_direction_feature_1 = innerprod(v.unsqueeze(1), normalized_atom_position_distances.unsqueeze(-1))\n",
    "        atomic_direction_feature_list.append(atomic_direction_feature_1)\n",
    "        # 就是每个元素想乘然后对第三维求\n",
    "    if self.use_dir2:\n",
    "        atomic_direction_feature_2 = innerprod(v.unsqueeze(2), normalized_atom_position_distances.unsqueeze(-1))\n",
    "        atomic_direction_feature_list.append(atomic_direction_feature_2)\n",
    "        # 两者添加的维度位置不同。\n",
    "    if self.use_dir3:\n",
    "        atomic_direction_feature_3 = innerprod(\n",
    "            self.q_proj(v).unsqueeze(1),\n",
    "            self.k_proj(v).unsqueeze(2))\n",
    "        atomic_direction_feature_list.append(atomic_direction_feature_3)\n",
    "    # dirs 里面的每个元素的维度都是 batch_size x atomic_number x atomic_number x hid_dim\n",
    "\n",
    "    # batch_size x atomic_number x atomic_number x 8(3+3+2) # 坐标，坐标，角度\n",
    "    combined_direction_features = torch.cat(atomic_direction_feature_list, dim=-1)  # batch_size x atomic_number x atomic_number x hid_dim x 2\n",
    "    # 这个就是把矩阵的最后一个维度进行拼接， 拼接的维度是 hid_dim * (use_dir1 + use_dir2 + use_dir3)\n",
    "    if self.ev_decay:\n",
    "        combined_direction_features = combined_direction_features * atomic_adjacency_matrix.unsqueeze(-1)\n",
    "    if self.add_ef2dir or self.no_filter_decomposition:\n",
    "        combined_direction_features = torch.cat((combined_direction_features, edge_features), dim=-1) # batch_size x atomic_number x atomic_number x (hid_dim * 2 + ef_dim)\n",
    "    if self.no_filter_decomposition:\n",
    "        mask = self.dir_proj(combined_direction_features)\n",
    "    else:\n",
    "        dir_project2_max = self.dir_proj(combined_direction_features)\n",
    "        mask = mask * dir_project2_max\n",
    "    # 然后就是把dir投影到mask的维度， 然后想乘， 带有广播机制的。\n",
    "    # 所以这几步的目的就是构建特征矩阵， 把前面的特征全部\n",
    "    # mask: batch_size x atomic_number x atomic_number x hid_dim => 还是类似于领接矩阵表示的边\n",
    "    # s: batch_size x atomic_number x hid_dim => 类似于节点。\n",
    "    for layer_idx, interaction in enumerate(self.interactions):\n",
    "        if self.no_share_filter:\n",
    "            mask = (self.ef_projections[layer_idx](edge_features) *\n",
    "                    self.dir_projections[layer_idx](combined_direction_features))\n",
    "            # mask的形状未改变。\n",
    "        s = interaction(s, mask) + s\n",
    "        # s的形状未改变。\n",
    "    s[atomic_numbers == 0] = 0\n",
    "    # s = self.output_module_1(s)\n",
    "    # s = s.squeeze(-1)\n",
    "    # s = self.output_module_activation(s)\n",
    "    # s = self.output_module_2(s)\n",
    "    # s = torch.sum(s, dim=1)\n",
    "    # s = self.output_module(s)\n",
    "    # s = s * self.y_std + self.y_mean\n",
    "    # print(f\"y_std: {self.y_std}, y_mean: {self.y_mean}\")\n",
    "    #  without this: Training (Epoch 42/6000):\n",
    "    #  100%|██████████| 8/8 [00:01<00:00,  5.91it/s, energy_loss=733, force_loss=25.8]\n",
    "    #  with this: Training (Epoch 42/6000):\n",
    "    #  100%|██████████| 8/8 [00:01<00:00,  6.00it/s, energy_loss=280, force_loss=26.8]\n",
    "    #  seems that work for force loss, 但是这里面的y_std 和 y_mean 并没有改变。 其实也没必要改变， 因为这里的y_std 和 y_mean。\n",
    "    return s, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f9b9eda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnnlf_model.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f423a1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "equvilent_output_module = output_modules.EquivariantScalar(hidden_channels=256)\n",
    "#  x = self.output_model.pre_reduce(x, v, data.z, data.pos, data.batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f212da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "s, mask = gnnlf_model.forward(self=gnnlf_model, atomic_numbers=z, atomic_positions=pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4ec76986",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_reduct_out = equvilent_output_module.pre_reduce(s, mask, None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e1546b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 21, 1])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_reduct_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e252a89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(pre_reduct_out, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "708d4101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 21, 256])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "afa37a68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 21, 21, 256])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6650d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "equvilent_output_module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc7bc6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a6220ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch_one' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mtype\u001b[39m(\u001b[43mbatch_one\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batch_one' is not defined"
     ]
    }
   ],
   "source": [
    "type(batch_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42cae75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "\n",
    "def gnn_lf_batch2visnet_adapter(batch):\n",
    "\n",
    "    z, pos, y, dy = batch\n",
    "    batch_size = pos.shape[0]\n",
    "    sep = pos.shape[1]\n",
    "    visnet_y = y\n",
    "    visnet_pos = pos.view(-1, pos.size(-1))\n",
    "    visnet_z = z.view(-1, )\n",
    "    visnet_batch = torch.arange(0, batch_size)\n",
    "    # repeat each of the value in the visnet_batch for sep times \n",
    "    visnet_batch = visnet_batch.repeat_interleave(sep)\n",
    "    visnet_dy = dy.view(-1, dy.size(-1))\n",
    "    ptr = torch.arange(0, batch_size * sep + 1, sep)\n",
    "    # conver it to DataBatch with key of \"y, pos, z, dy, batch, ptr\"\n",
    "    # init a torch_geometric.data.batch.DataBatch\n",
    "    _batch = torch_geometric.data.Batch(y=visnet_y, pos=visnet_pos, z=visnet_z, dy=visnet_dy, batch=visnet_batch, ptr=ptr)\n",
    "    return _batch    \n",
    "    # from torch \n",
    "    # torch_geometric.data.batch.DataBatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "184308a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(y=[17, 1], pos=[357, 3], z=[357], dy=[357, 3], batch=[357], ptr=[18])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnn_lf_batch2visnet_adapter(first_train_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1c8621d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch_geometric.data.batch.DataBatch"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(gnn_lf_batch2visnet_adapter(first_train_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c9fe35e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.9876],\n",
       "         [-1.2070],\n",
       "         [-1.3699],\n",
       "         [-1.7094],\n",
       "         [-2.4913],\n",
       "         [-1.9085],\n",
       "         [-2.2696],\n",
       "         [-1.8223],\n",
       "         [-2.0106],\n",
       "         [-2.4089],\n",
       "         [-2.0464],\n",
       "         [-2.0982],\n",
       "         [-1.7267],\n",
       "         [-1.9202],\n",
       "         [-2.1074],\n",
       "         [-1.9732],\n",
       "         [-1.9028]], grad_fn=<AddBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(gnn_lf_batch2visnet_adapter(first_train_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15428ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(gnn_lf_batch2visnet_adapter(first_train_batch))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "940e3b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = gnnlf_model(z, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "008ee4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2601],\n",
       "        [-0.2455],\n",
       "        [-0.1928],\n",
       "        [-0.2315],\n",
       "        [-0.1238],\n",
       "        [-0.1599],\n",
       "        [-0.2234],\n",
       "        [-0.2103],\n",
       "        [-0.1576],\n",
       "        [-0.2259],\n",
       "        [-0.2741],\n",
       "        [-0.1637],\n",
       "        [-0.1835],\n",
       "        [-0.2287],\n",
       "        [-0.2016],\n",
       "        [-0.2201],\n",
       "        [-0.1845]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d8229591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf304f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
