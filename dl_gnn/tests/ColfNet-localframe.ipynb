{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26fe91bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "# 将上级目录的路径添加到sys.path中\n",
    "sys.path.append(r\"C:\\Users\\23174\\Desktop\\GitHub Project\\GitHubProjectBigData\\GNN-Molecular-Project\\GNN-LF-AND-ColfNet\")\n",
    "\n",
    "# 现在你可以导入上级目录中的模块了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c566f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from colorama import Fore, Back, Style\n",
    "from dl_gnn.models.impl.GNNLF import GNNLF\n",
    "from dl_gnn.models.impl.ThreeDimFrame import GNNLF as ThreeDGNNLF\n",
    "from dl_gnn.configs.path_configs import OUTPUT_PATH\n",
    "from dl_gnn.models.impl import Utils\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR\n",
    "from torch.nn.functional import l1_loss, mse_loss\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from warnings import filterwarnings\n",
    "from Dataset import load\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "energy_loss_weight = 0.01  # the ratio of energy loss\n",
    "force_loss_weight = 1  # ratio of force loss\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "def init_model(y_mean, y_std, global_y_mean, **kwargs):\n",
    "    if False:\n",
    "        model = ThreeDGNNLF(y_mean=y_mean,\n",
    "                            y_std=y_std,\n",
    "                            global_y_mean=global_y_mean,\n",
    "                            **kwargs)\n",
    "    else:\n",
    "        model = GNNLF(**kwargs)\n",
    "    print(f\"numel {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "    # wandb.log({\"numel\": sum(p.numel() for p in model.parameters() if p.requires_grad)})\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_dataset(dataset_name: str):\n",
    "    # for args.dataset in ['uracil', 'naphthalene', 'aspirin', 'salicylic',  'malonaldehyde', 'ethanol', 'toluene', 'benzene']:\n",
    "    dataset = load(dataset_name)\n",
    "\n",
    "    N = dataset[0].z.shape[0]\n",
    "    global_y_mean = torch.mean(dataset.data.y)\n",
    "    dataset.data.y = (dataset.data.y - global_y_mean).to(torch.float32)\n",
    "    tensor_dataset = TensorDataset(dataset.data.z.reshape(-1, N),\n",
    "                        dataset.data.pos.reshape(-1, N, 3),\n",
    "                        dataset.data.y.reshape(-1, 1),\n",
    "                        dataset.data.dy.reshape(-1, N, 3))\n",
    "    meta_data = {\n",
    "        \"global_y_mean\": global_y_mean,\n",
    "    }\n",
    "    return tensor_dataset, meta_data\n",
    "\n",
    "\n",
    "def train(learning_rate: float = 1e-3,\n",
    "          initial_learning_rate_ratio: float = 1e-1,\n",
    "          minimum_learning_rate_ratio: float = 1e-3,\n",
    "          epoches: int = 3000,\n",
    "          save_model: bool = False,\n",
    "          enable_testing: bool = False,\n",
    "          is_training: bool = False,\n",
    "          search_hp: bool = False,\n",
    "          max_early_stop_steps: int = 500,\n",
    "          patience: int = 10,\n",
    "          warmup: int = 30,\n",
    "          **kwargs):\n",
    "    tensor_dataset, meta_data = load_dataset(args.dataset)\n",
    "    NAN_PANITY = 1e1\n",
    "    # TODO: 这个写法不对吧， 他训练集只有几百个， 测试集有200000。\n",
    "    # if search_hp:\n",
    "    #     train_dataset, validation_dataset, test_dataset = random_split(tensor_dataset,\n",
    "    #                                                                    [950, 256, len(tensor_dataset)-950-256])\n",
    "    # elif args.gemnet_split:\n",
    "    #     train_dataset, validation_dataset, test_dataset = random_split(tensor_dataset,\n",
    "    #                                                                    [1000, 1000, len(tensor_dataset)-2000])\n",
    "    # else:\n",
    "    if True:\n",
    "        train_dataset, validation_dataset, test_dataset = random_split(tensor_dataset,\n",
    "                                                                       [950, 50, len(tensor_dataset)-1000])\n",
    "    # train_dataset, validation_dataset, test_dataset = random_split(tensor_dataset,\n",
    "    #                                                                [int(0.7*len(tensor_dataset)),\n",
    "    #                                                                  int(0.2*len(tensor_dataset)),\n",
    "    #                                                                  len(tensor_dataset)-int(0.7*len(tensor_dataset))-int(0.2*len(tensor_dataset))])\n",
    "\n",
    "    validation_dataloader = DataLoader(validation_dataset, batch_size=args.validation_batch_size, shuffle=False)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False)\n",
    "    # 0.1*len(tensor_dataset)])\n",
    "    # validation_dataloader = DataLoader(validation_dataset, batch_size=len(validation_dataset), shuffle=False)\n",
    "    # TODO: change the batch size of train_dataset so that the VRAM is enough\n",
    "    # train_dataloader = DataLoader(train_dataset, batch_size=len(train_dataset)//30, shuffle=False)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=False)\n",
    "    train_data_size = len(train_dataset)\n",
    "\n",
    "    # trn_dl = Utils.tensorDataloader(train_dataloader, batch_size, True, device)\n",
    "    train_batch_one = next(iter(train_dataloader))\n",
    "    _, atomic_number = train_batch_one[0].shape\n",
    "    if kwargs.get(\"atomic_number\", None) is None:\n",
    "        kwargs[\"atomic_number\"] = atomic_number\n",
    "    y_mean = torch.mean(train_batch_one[2]).item()\n",
    "    y_std = torch.std(train_batch_one[2]).item()\n",
    "    global_y_mean = meta_data[\"global_y_mean\"]\n",
    "\n",
    "\n",
    "    model = init_model(y_mean=y_mean, y_std=y_std, global_y_mean=global_y_mean, **kwargs).to(device)\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_train_loss = float(\"inf\")\n",
    "    if not is_training:\n",
    "        opt = Adam(model.parameters(), lr=learning_rate * initial_learning_rate_ratio if warmup > 0 else learning_rate)\n",
    "        scd1 = StepLR(\n",
    "            opt,\n",
    "            1,\n",
    "            gamma=(1 / initial_learning_rate_ratio)**(1 / (warmup * (train_data_size // args.train_batch_size))) if warmup > 0 else 1)\n",
    "        # 用于warmup阶段\n",
    "        scd = ReduceLROnPlateau(opt,\n",
    "                                \"min\",\n",
    "                                0.8,\n",
    "                                patience=patience,\n",
    "                                min_lr=learning_rate * minimum_learning_rate_ratio,\n",
    "                                threshold=0.0001)\n",
    "        # 用于正常训练阶段\n",
    "        early_stop = 0\n",
    "        tqdm_epochs = tqdm(range(epoches), desc=\"Epochs\")\n",
    "        for epoch in tqdm_epochs:\n",
    "            current_learning_rate = opt.param_groups[0][\"lr\"]\n",
    "            training_losses = [[], []]\n",
    "            start_time = time.time()\n",
    "            tqdm_object = tqdm(train_dataloader, desc=f\"Training (Epoch {epoch + 1}/{epoches})\")\n",
    "            for train_batch in tqdm_object:\n",
    "                train_batch = [_.to(device) for _ in train_batch]\n",
    "                training_energy_loss, training_force_loss = Utils.train_grad(\n",
    "                    train_batch, opt, model, mse_loss, energy_loss_weight, force_loss_weight\n",
    "                )\n",
    "                if np.isnan(training_force_loss):\n",
    "                    return NAN_PANITY\n",
    "                training_losses[0].append(training_energy_loss)\n",
    "                training_losses[1].append(training_force_loss)\n",
    "                # Calculate the average loss for the current batch or over a window\n",
    "                average_energy_loss = np.mean(training_losses[0][-10:])  # Example: average over the last 10 batches\n",
    "                average_force_loss = np.mean(training_losses[1][-10:])\n",
    "                # Update tqdm progress bar with the current loss values\n",
    "                tqdm_object.set_postfix(energy_loss=average_energy_loss, force_loss=average_force_loss)\n",
    "                if epoch < warmup:\n",
    "                    scd1.step()\n",
    "\n",
    "            time_cost = time.time() - start_time\n",
    "            training_energy_loss = np.average(training_losses[0])\n",
    "            training_force_loss = np.average(training_losses[1])\n",
    "            train_loss = energy_loss_weight * training_energy_loss + training_force_loss\n",
    "            validation_energy_loss, validation_force_loss = Utils.test_grad(validation_dataloader, model, l1_loss)\n",
    "            val_loss = energy_loss_weight * validation_energy_loss + validation_force_loss\n",
    "            early_stop += 1\n",
    "            scd.step(val_loss)\n",
    "            if np.isnan(val_loss):\n",
    "                return NAN_PANITY\n",
    "            if val_loss < best_val_loss:\n",
    "                print(f\"current loss {val_loss} is better than best loss {best_val_loss}\")\n",
    "                early_stop = 0\n",
    "                best_val_loss = val_loss\n",
    "                if save_model:\n",
    "                    torch.save(model.state_dict(), model_save_path)\n",
    "            if train_loss < best_train_loss:\n",
    "                best_train_loss = train_loss\n",
    "\n",
    "            if early_stop > max_early_stop_steps:\n",
    "                break\n",
    "            print(\n",
    "                f\"Epoch: {epoch}, Time elapsed: {time_cost} seconds, \"\n",
    "                f\"Learning rate: {current_learning_rate:.4e}, \"\n",
    "                f\"Training energy loss: {training_energy_loss:.4f}, Training force loss: {training_force_loss:.4f}, \"\n",
    "                f\"Validation energy loss: {validation_energy_loss:.4f}, Validation force loss: {validation_force_loss:.4f}\"\n",
    "            )\n",
    "\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"time(s)\": time_cost,\n",
    "                \"learning rate\": current_learning_rate,\n",
    "                \"training energy loss\": training_energy_loss,\n",
    "                \"training force loss\": training_force_loss,\n",
    "                \"validation energy loss\": validation_energy_loss,\n",
    "                \"validation force loss\": validation_force_loss,\n",
    "            })\n",
    "            wandb.log({\n",
    "                \"best val loss\": best_val_loss,\n",
    "                \"best train loss\": best_train_loss\n",
    "            })\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(\"\", end=\"\", flush=True)\n",
    "            if training_force_loss > 1000:\n",
    "                return min(best_val_loss, NAN_PANITY)\n",
    "\n",
    "    if enable_testing:\n",
    "        model.load_state_dict(torch.load(model_save_path, map_location=\"cpu\"))\n",
    "        mod = model.to(device)\n",
    "        # tst_score = []\n",
    "        # num_mol = []\n",
    "        # for batch in test_dataloader:\n",
    "        #     num_mol.append(batch[0].shape[0])\n",
    "        #     batch = tuple(_.to(device) for _ in batch)\n",
    "        #     tst_score.append(Utils.test_grad(batch, mod, l1_loss))\n",
    "        #     # print(f\"test score {tst_score[-1]}\")\n",
    "        # num_mol = np.array(num_mol)\n",
    "        tst_score = Utils.test_grad(test_dataloader, mod, l1_loss, show_progress_bar=True)\n",
    "        # tst_score = np.sum(tst_score * (num_mol.reshape(-1, 1) / num_mol.sum()), axis=0)\n",
    "        # tst_score = None\n",
    "        trn_score = Utils.test_grad(train_dataloader, mod, l1_loss)\n",
    "        val_score = Utils.test_grad(validation_dataloader, mod, l1_loss)\n",
    "        print(trn_score, val_score, tst_score)\n",
    "        wandb.log({\n",
    "            \"training score\": trn_score,\n",
    "            \"validation score\": val_score,\n",
    "            \"test score\": tst_score\n",
    "        })\n",
    "    print(\"best val loss\", best_val_loss)\n",
    "    wandb.log({\"best val loss\": best_val_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "10dea905",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    pass\n",
    "\n",
    "args = Args()\n",
    "args.validation_batch_size = 60\n",
    "args.test_batch_size = 60\n",
    "args.train_batch_size = 60\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06d22d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_dataset, meta_data = load_dataset(\"aspirin\")\n",
    "NAN_PANITY = 1e1\n",
    "# TODO: 这个写法不对吧， 他训练集只有几百个， 测试集有200000。\n",
    "# if search_hp:\n",
    "#     train_dataset, validation_dataset, test_dataset = random_split(tensor_dataset,\n",
    "#                                                                    [950, 256, len(tensor_dataset)-950-256])\n",
    "# elif args.gemnet_split:\n",
    "#     train_dataset, validation_dataset, test_dataset = random_split(tensor_dataset,\n",
    "#                                                                    [1000, 1000, len(tensor_dataset)-2000])\n",
    "# else:\n",
    "if True:\n",
    "    train_dataset, validation_dataset, test_dataset = random_split(tensor_dataset,\n",
    "                                                                    [950, 50, len(tensor_dataset)-1000])\n",
    "# train_dataset, validation_dataset, test_dataset = random_split(tensor_dataset,\n",
    "#                                                                [int(0.7*len(tensor_dataset)),\n",
    "#                                                                  int(0.2*len(tensor_dataset)),\n",
    "#                                                                  len(tensor_dataset)-int(0.7*len(tensor_dataset))-int(0.2*len(tensor_dataset))])\n",
    "\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=args.validation_batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False)\n",
    "# 0.1*len(tensor_dataset)])\n",
    "# validation_dataloader = DataLoader(validation_dataset, batch_size=len(validation_dataset), shuffle=False)\n",
    "# TODO: change the batch size of train_dataset so that the VRAM is enough\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=len(train_dataset)//30, shuffle=False)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=False)\n",
    "train_data_size = len(train_dataset)\n",
    "\n",
    "# trn_dl = Utils.tensorDataloader(train_dataloader, batch_size, True, device)\n",
    "train_batch_one = next(iter(train_dataloader))\n",
    "_, atomic_number = train_batch_one[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "308f1a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 train_dataloader 是一个 DataLoader 实例\n",
    "train_dataloader_iter = iter(train_dataloader)\n",
    "train_batch = next(train_dataloader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf594b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_number_batch, position_batch, energy_batch, force_batch = train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f4695a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 21, 3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_batch.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "81ef5f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 1, 3])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid = torch.mean(position_batch, dim=1, keepdim=True)\n",
    "centroid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1cb181e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 21, 3])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_batch_center = (position_batch - centroid)\n",
    "position_batch_center.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "93a8ed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from md17_params import get_md17_params\n",
    "from dl_gnn.tests.md17_params import get_md17_params\n",
    "params = get_md17_params(\"aspirin\")\n",
    "params[\"use_dir2\"] = True\n",
    "params[\"use_dir3\"] = True\n",
    "params[\"global_frame\"] = True\n",
    "params[\"no_filter_decomp\"] = True\n",
    "params[\"nolin1\"] = True\n",
    "params[\"no_share_filter\"] = True\n",
    "params[\"is_training\"] = True\n",
    "params[\"threedframe\"] = True\n",
    "params[\"atomic_number\"] = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "939f76b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numel 3713784\n"
     ]
    }
   ],
   "source": [
    "model = init_model(y_mean=0, y_std=0, global_y_mean=0, **params).to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b5b34011",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(atomic_number_batch, position_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "af68b473",
   "metadata": {},
   "outputs": [],
   "source": [
    "(atomic_number_embedding,\n",
    " atomic_adjacency_matrix,\n",
    " normalized_atom_position_distances,\n",
    " edge_features\n",
    " ) = (model.mol2graph(atomic_number_batch, position_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "455de30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 21, 21])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atomic_adjacency_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9a9cfec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coord2localframe_batch_corrected(position_batch_center, atomic_adjacency_matrix, norm_diff=True):\n",
    "    batch_size, num_atoms, _ = position_batch_center.shape\n",
    "    \n",
    "    # 扩展维度以进行广播\n",
    "    coords_row = position_batch_center.unsqueeze(2).expand(-1, -1, num_atoms, -1)\n",
    "    coords_col = position_batch_center.unsqueeze(1).expand(-1, num_atoms, -1, -1)\n",
    "    \n",
    "    # 计算坐标差异\n",
    "    coord_diff = coords_row - coords_col\n",
    "    \n",
    "    # 计算径向距离\n",
    "    radial = torch.sum(coord_diff ** 2, dim=-1, keepdim=True)\n",
    "    \n",
    "    # 计算叉乘\n",
    "    coord_cross = torch.cross(coord_diff, coord_diff, dim=-1)  # 这里应保持coord_diff作为叉乘的输入\n",
    "    \n",
    "    # 规范化向量\n",
    "    if norm_diff:\n",
    "        norm = torch.sqrt(radial) + 1\n",
    "        coord_diff = coord_diff / norm\n",
    "        cross_norm = torch.sqrt(torch.sum(coord_cross ** 2, dim=-1, keepdim=True)) + 1\n",
    "        coord_cross = coord_cross / cross_norm\n",
    "    \n",
    "    # 计算第三个轴\n",
    "    coord_vertical = torch.cross(coord_diff, coord_cross, dim=-1)\n",
    "    \n",
    "    # 使用邻接矩阵筛选出相邻的原子对\n",
    "    mask = atomic_adjacency_matrix.unsqueeze(-1)  # 扩展维度以匹配\n",
    "    coord_diff = coord_diff * mask\n",
    "    coord_cross = coord_cross * mask\n",
    "    coord_vertical = coord_vertical * mask\n",
    "    \n",
    "    return coord_diff, coord_cross, coord_vertical\n",
    "\n",
    "\n",
    "coord_diff, coord_cross, coord_vertical = coord2localframe_batch_corrected(position_batch_center, atomic_adjacency_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2ea04b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 21, 21, 3])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_diff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "85a37e7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 21, 21, 3])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_cross.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "666cda80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 21, 21, 3])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_vertical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "962f32ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 21, 21, 3, 3])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e878bac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 21, 21, 1, 3])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_vertical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "67ec59d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 21, 21, 2])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudo_angle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "4afd816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def scalarization_batch(position_batch_center, atomic_adjacency_matrix):\n",
    "    coord_diff, coord_cross, coord_vertical = coord2localframe_batch_corrected(position_batch_center, atomic_adjacency_matrix, norm_diff=True)\n",
    "    batch_size, num_atoms, _, _ = coord_diff.shape\n",
    "    coord_diff = coord_diff.unsqueeze(-2)\n",
    "    coord_cross = coord_cross.unsqueeze(-2)\n",
    "    coord_vertical = coord_vertical.unsqueeze(-2)\n",
    "    # 由于已经是批处理数据，edges参数不再需要，所有操作都是基于批处理和邻接矩阵\n",
    "    # 合并局部坐标框架向量\n",
    "    edge_basis = torch.cat([coord_diff, coord_cross, coord_vertical], dim=-2)  # 修改dim参数以正确合并\n",
    "    # edge_basis.shape\n",
    "    # r_i 和 r_j 的计算需要调整为适应批处理数据\n",
    "    # 这里直接使用position_batch_center，因为我们已经有了所有原子对的局部坐标框架向量\n",
    "    r_i = position_batch_center.unsqueeze(2).expand(-1, -1, num_atoms, -1)  # [batch_size, num_atoms, num_atoms, 3]\n",
    "    r_j = position_batch_center.unsqueeze(1).expand(-1, num_atoms, -1, -1)  # [batch_size, num_atoms, num_atoms, 3]\n",
    "\n",
    "    coff_i = torch.matmul(edge_basis, r_i.unsqueeze(-1)).squeeze(-1)  \n",
    "    coff_j = torch.matmul(edge_basis, r_j.unsqueeze(-1)).squeeze(-1)  \n",
    "\n",
    "    # 计算角度信息\n",
    "    coff_mul = coff_i * coff_j\n",
    "    coff_i_norm = torch.norm(coff_i, dim=-1, keepdim=True) + 1e-5\n",
    "    coff_j_norm = torch.norm(coff_j, dim=-1, keepdim=True) + 1e-5\n",
    "    pseudo_cos = coff_mul.sum(dim=-1, keepdim=True) / (coff_i_norm * coff_j_norm)\n",
    "    pseudo_sin = torch.sqrt(1 - pseudo_cos.pow(2))\n",
    "    pseudo_angle = torch.cat([pseudo_sin, pseudo_cos], dim=-1)\n",
    "\n",
    "    # 合并特征\n",
    "    coff_feat = torch.cat([pseudo_angle, coff_i, coff_j], dim=-1)\n",
    "    # coff_feat.shape\n",
    "    return coff_feat\n",
    "coff_feat = scalarization_batch(position_batch_center, atomic_adjacency_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9ab2b49e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 21, 21, 8])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coff_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102419a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_basis, r_i.unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "85143ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 21, 21, 9])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_diff, coord_cross, coord_vertical = coord2localframe_batch_corrected(position_batch_center, atomic_adjacency_matrix, norm_diff=True)\n",
    "\n",
    "# 由于已经是批处理数据，edges参数不再需要，所有操作都是基于批处理和邻接矩阵\n",
    "# 合并局部坐标框架向量\n",
    "edge_basis = torch.cat([coord_diff, coord_cross, coord_vertical], dim=-1)  # 修改dim参数以正确合并\n",
    "\n",
    "# r_i 和 r_j 的计算需要调整为适应批处理数据\n",
    "# 这里直接使用position_batch_center，因为我们已经有了所有原子对的局部坐标框架向量\n",
    "r_i = position_batch_center.unsqueeze(2)  # 增加一个维度以便广播\n",
    "r_j = position_batch_center.unsqueeze(1)  # 增加一个维度以便广播\n",
    "edge_basis.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "c683aa3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 21, 1, 3])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4f66b389",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [1260, 9] but got: [1260, 1].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[109], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m coff_i \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43medge_basis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_i\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [1260, 9] but got: [1260, 1]."
     ]
    }
   ],
   "source": [
    "coff_i = torch.matmul(edge_basis, r_i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3136aa0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5481bd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 21, 21, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 扩展维度以进行广播\n",
    "expanded_positions = position_batch_center.unsqueeze(2)  # [batch_size, num_atoms, 1, 3]\n",
    "# 复制所有坐标以形成所有可能的配对\n",
    "coords_row = expanded_positions.expand(-1, -1, 21, -1)\n",
    "coords_col = expanded_positions.transpose(1, 2).expand(-1, -1, 21, -1)\n",
    "\n",
    "# 计算坐标差异\n",
    "coord_diffs = coords_row - coords_col  # [batch_size, num_atoms, num_atoms, 3]\n",
    "\n",
    "# 使用邻接矩阵来筛选出存在的边\n",
    "# 注意：这将包含很多零向量，因为我们计算了所有可能的配对差异\n",
    "# 你可能需要进一步处理来仅保留实际存在的边\n",
    "real_edges_diffs = coord_diffs * atomic_adjacency_matrix.unsqueeze(-1)\n",
    "\n",
    "# 检查结果的形状\n",
    "print(real_edges_diffs.shape)  # 预期输出：[batch_size, num_atoms, num_atoms, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba930510",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def scalarization(self, edges, x):\n",
    "        coord_diff, coord_cross, coord_vertical = self.coord2localframe(edges, x)\n",
    "        # Geometric Vectors Scalarization\n",
    "        row, col = edges\n",
    "        edge_basis = torch.cat([coord_diff, coord_cross, coord_vertical], dim=1) \n",
    "        r_i = x[row]  \n",
    "        r_j = x[col]\n",
    "        coff_i = torch.matmul(edge_basis, r_i.unsqueeze(-1)).squeeze(-1)  \n",
    "        coff_j = torch.matmul(edge_basis, r_j.unsqueeze(-1)).squeeze(-1)  \n",
    "        # Calculate angle information in local frames\n",
    "        coff_mul = coff_i * coff_j  # [E, 3]\n",
    "        coff_i_norm = coff_i.norm(dim=-1, keepdim=True) + 1e-5\n",
    "        coff_j_norm = coff_j.norm(dim=-1, keepdim=True) + 1e-5\n",
    "        pesudo_cos = coff_mul.sum(dim=-1, keepdim=True) / coff_i_norm / coff_j_norm\n",
    "        pesudo_sin = torch.sqrt(1 - pesudo_cos**2)\n",
    "        pesudo_angle = torch.cat([pesudo_sin, pesudo_cos], dim=-1)\n",
    "        coff_feat = torch.cat([pesudo_angle, coff_i, coff_j], dim=-1)\n",
    "        return coff_feat\n",
    "\n",
    "    def forward(self, h, x, edges, vel, edge_attr, node_attr=None, n_nodes=5):\n",
    "        h = self.embedding_node(h)\n",
    "        x = x.reshape(-1, n_nodes, 3)\n",
    "        centroid = torch.mean(x, dim=1, keepdim=True)\n",
    "        x_center = (x - centroid).reshape(-1, 3)\n",
    "        coff_feat = self.scalarization(edges, x_center)\n",
    "        edge_feat = torch.cat([edge_attr, coff_feat], dim=-1)\n",
    "        edge_feat = self.fuse_edge(edge_feat)\n",
    "\n",
    "        for i in range(0, self.n_layers):\n",
    "            h, x_center, _ = self._modules[\"gcl_%d\" % i](\n",
    "                h, edges, x_center, vel, edge_attr=edge_feat, node_attr=node_attr)\n",
    "        # h 可能可以用于作为最后的输出的结果。\n",
    "        x = x_center.reshape(-1, n_nodes, 3) + centroid\n",
    "        x = x.reshape(-1, 3)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6a906cb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 21, 21, 3])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_atom_position_distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f3fd318c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 21, 256])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atomic_number_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2e6f98d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0370e995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
